# Python CircleCI 2.0 configuration file
#
# Check https://circleci.com/docs/2.0/language-python/ for more details
#
version: 2.1

orbs: 
  aws-eks: circleci/aws-eks@2.1.0
  kubernetes: circleci/kubernetes@1.0


commands:
  # TODO: adapt to project
  # destroy-eks-cluster-stack:
  #   description: Destroy EKS Cluster Stack.
  #   steps:
  #     - run:
  #         name: Destroy environments
  #         when: on_fail
  #         command: |
  #           # TODO: check first if pods are in cluster -> delete namespace & wait until done
  #           # kubectl delete namespace ml-app
  #           # TODO: check if EC2 Loadbalancer exists -> delete loadbalancer & wait until done
  #           # TODO: check if security group for Cluster exists -> delete security group
  #           CLUSTER_SG=aws ec2 describe-security-groups \
  #           --region ${AWS_REGION} \
  #           --filters Name=tag:Name,Values=${EKS_CLUSTER_NAME} \
  #           --query "SecurityGroups[*].{ID:GroupId}" \
  #           --output text
  #           if [ -z "$CLUSTER_SG" ]
  #           then
  #                 echo "\$CLUSTER_SG is NULL"
  #           else
  #                 echo "Deleting \$CLUSTER_SG ${CLUSTER_SG}..."; aws ec2 delete-security-group ${CLUSTER_SG}
  #           fi
  #           # 
  #           # TODO: check if Cluster exists -> delete Cluster & wait until done
  #           # eksctl delete cluster --name ${EKS_CLUSTER_NAME} --region ${AWS_REGION}
  #           # TODO: check if Cloudformation stack exists -> delete cloudformation stacks & wait until done
  #           aws cloudformation delete-stack \
  #           --stack-name eksctl-ML-APP-CLUSTER-cluster

  destroy-fargate-profile:
    description: Destroy Fargate profile.
    steps:
      - run:
          name: Destroy environments
          when: on_fail
          command: |
            eksctl delete fargateprofile \
                --name fargate_profile_ml-app-${CIRCLE_WORKFLOW_ID:0:7} \
                --cluster ${EKS_CLUSTER_NAME} \
                --region ${AWS_REGION}
            # wait for 3 minutes since only one fargate profile can be deleted at the same time
            sleep 3m

jobs:
  build_app:
    docker:
      # Use the same Docker base as the project
      - image: python:3.7.3-stretch
    working_directory: ~/repo
    steps:
      - checkout
      # Download and cache dependencies
      - restore_cache:
          keys:
          - v1-dependencies-{{ checksum "requirements.txt" }}
          # fallback to using the latest cache if no exact match is found
          - v1-dependencies-
      - run:
          name: install dependencies
          command: |
            python3 -m venv venv
            . venv/bin/activate
            echo "Virtual Environment created, now installing requirements..."
            make install
            # Install hadolint
            echo "Requirements installed, now installing hadolint..."
            wget -O /bin/hadolint https://github.com/hadolint/hadolint/releases/download/v1.16.3/hadolint-Linux-x86_64 &&\
            chmod +x /bin/hadolint
      - save_cache:
          paths:
          - ./venv
          key: v1-dependencies-{{ checksum "requirements.txt" }}
      # run lint
      - run:
          name: run lint
          command: |
            . venv/bin/activate
            make lint

  # ToDo
  # OPTIONAL
  # create unit test of app 
  # run app locally and send dummy request to API
  # use unit test to verify expected result
  # test_app:

  # ToDo
  # store in AWS image repository -> AWS ECR
  build_container:
    docker:
      - image: cimg/base:2021.04
      # - image: docker:17.05.0-ce-git
    steps:
      - checkout
      # - setup_remote_docker:
      #     version: 19.03.13
      #     docker_layer_caching: true
      - setup_remote_docker
      - run:
          name: build docker image
          command: |
            docker build --tag=ml-app-local .
      - run:
          name: install dependencies
          command: |
            sudo apt-get update
            sudo apt-get install --upgrade awscli
            echo 'aws cli version:'
            aws --version   
      - run:
          name: authenticate and upload docker image to AWS ECR
          command: |
            # to local docker image to central path
            docker image tag ml-app-local:latest $DOCKERPATH
            # Retrieve an authentication token and authenticate your Docker client to your registry.
            aws ecr get-login-password --region ${AWS_REGION} | docker login --username AWS --password-stdin ${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com
            # Push Docker image to AWS ECR repository
            docker push $DOCKERPATH

  # create AWS Kubernetes Cluster -> AWS EKS
  create_eks_cluster:  
    docker:
      - image: amazon/aws-cli
    steps:
      - checkout
      - run:
          name: Install dependencies
          command: |
            # install tar utility
            yum install -y tar gzip
      - aws-eks/setup:
          version: 0.80.0
      - kubernetes/install
      - run:
          name: check installs
          command: |
            eksctl version
            kubectl config view
      - run:
          name: created managed Kubernetes cluster via AWS Fargate
          command: |
            CLUSTERS=($(aws eks list-clusters --query "clusters" --output text))
            if [[ ! " ${CLUSTERS[@]} " =~ " ${EKS_CLUSTER_NAME} " ]]
            then
              echo "Creating EKS Fargate cluster..."
              # create managed cluster on AWS Fargate
              eksctl create cluster \
              --name ${EKS_CLUSTER_NAME} \
              --region ${AWS_REGION} \
              --zones ${AWS_REGION}a,${AWS_REGION}b \
              --fargate 
              # View Kubernetes resources
              kubectl get nodes -o wide
              # View the workloads running on the cluster
              kubectl get pods --all-namespaces -o wide
            else
              echo "--- Existing EKS cluster ${EKS_CLUSTER_NAME} - no need for cluster creation ---"
            fi

  setup_eks_loadbalancer: 
    docker:
      - image: amazon/aws-cli
    steps:
      - checkout
      - run:
          name: Install dependencies
          command: |
            # install tar utility
            yum install -y tar gzip
            # install openssl
            yum install -y openssl
            # install git, required for step "Install the TargetGroupBinding CRDs"
            yum install -y git
      - aws-eks/setup:
          version: 0.80.0
      - kubernetes/install
      - run:
          name: install helm
          command: |
            curl -sSL https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash
            helm version --short
            helm repo add stable https://charts.helm.sh/stable
            # helm completion bash >> ~/.bash_completion
            # . /etc/profile.d/bash_completion.sh
            # . ~/.bash_completion
            # source <(helm completion bash)
      - run:
          name: check installs
          command: |
            helm version
            eksctl version
            kubectl config view
      - run:
          name: Create a kubeconfig for Amazon EKS 
          command: |
            # manual: -> https://docs.aws.amazon.com/eks/latest/userguide/create-kubeconfig.html
            aws eks update-kubeconfig --region ${AWS_REGION} --name ${EKS_CLUSTER_NAME}
            # test configuration
            kubectl get pods --all-namespaces -o wide
      - run:
          name: create IAM OIDC provider
          command: |
            eksctl utils associate-iam-oidc-provider \
            --region ${AWS_REGION} \
            --cluster ${EKS_CLUSTER_NAME} \
            --approve
      - run:
          name: create IAM policy
          command: |
            # check if policy exists
            if aws iam list-policies --scope Local --output text | grep "AWSLoadBalancerControllerIAMPolicy"
              then
                echo "IAM policy already exists"
              else
                curl -o iam_policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.2.0/docs/install/iam_policy.json
                aws iam create-policy \
                    --policy-name AWSLoadBalancerControllerIAMPolicy \
                    --policy-document file://iam_policy.json
                rm iam_policy.json
            fi  
      - run:
          name: Create a IAM role and ServiceAccount for the Load Balancer controller
          command: |
            eksctl create iamserviceaccount \
            --cluster ${EKS_CLUSTER_NAME} \
            --namespace kube-system \
            --name aws-load-balancer-controller \
            --attach-policy-arn arn:aws:iam::${AWS_ACCOUNT_ID}:policy/AWSLoadBalancerControllerIAMPolicy \
            --override-existing-serviceaccounts \
            --region ${AWS_REGION} \
            --approve
      - run:
          name: Install the TargetGroupBinding CRDs
          command: |
            kubectl apply -k "github.com/aws/eks-charts/stable/aws-load-balancer-controller/crds?ref=master"
      - run:
          name: Deploy the Helm chart from the Amazon EKS charts repo
          command: |
            # verify if AWS Load Balancer version has been set -> https://www.eksworkshop.com/020_prerequisites/k8stools/#set-the-aws-load-balancer-controller-version
            echo "Verifying LBC version..."
            if [ ! -x ${LBC_VERSION} ]
              then
                echo 'LBC_VERSION ${LBC_VERSION} has been set.'
              else
                echo 'LBC_VERSION has NOT been set.'
            fi
            # deploy Helm chart
            echo "Deploying Helm chart..."
            helm repo add eks https://aws.github.io/eks-charts
            echo "Exporting VPC ID..."
            export VPC_ID=$(aws eks describe-cluster \
                            --name ${EKS_CLUSTER_NAME} \
                            --query "cluster.resourcesVpcConfig.vpcId" \
                            --region ${AWS_REGION} \
                            --output text)
            echo "Upgrading ALB via Helm..."
            helm upgrade -i aws-load-balancer-controller \
                eks/aws-load-balancer-controller \
                -n kube-system \
                --set clusterName=${EKS_CLUSTER_NAME} \
                --set serviceAccount.create=false \
                --set serviceAccount.name=aws-load-balancer-controller \
                --set image.tag="${LBC_VERSION}" \
                --set region=${AWS_REGION} \
                --set vpcId=${VPC_ID}
      - run:
          name: Check if deployment has completed
          command: |
            kubectl -n kube-system rollout status deployment aws-load-balancer-controller

  create_fargate_profile:
    # "${CIRCLE_WORKFLOW_ID:0:7}"  
    docker:
      - image: amazon/aws-cli
    steps:
      - checkout
      - run:
          name: Install dependencies
          command: |
            # install tar utility
            yum install -y tar gzip
      - aws-eks/setup:
          version: 0.80.0
      - kubernetes/install
      - run:
          name: check installs
          command: |
            eksctl version
            kubectl config view
      - run:
          name: workflow ID specific fargate_profile
          command: |
            # create Fargate profile ml-app -> see https://docs.aws.amazon.com/eks/latest/userguide/fargate-profile.html
            eksctl create fargateprofile \
            --cluster ${EKS_CLUSTER_NAME} \
            --name fargate_profile_ml-app-${CIRCLE_WORKFLOW_ID:0:7} \
            --namespace ml-app-${CIRCLE_WORKFLOW_ID:0:7} \
            --region ${AWS_REGION}
            # verify that profile was created
            aws eks list-fargate-profiles \
            --cluster-name ${EKS_CLUSTER_NAME} \
            --region ${AWS_REGION}


  deploy_container: 
    docker:
      - image: amazon/aws-cli
    steps:
      - checkout
      - kubernetes/install
      - run:
          name: Create a kubeconfig for Amazon EKS 
          command: |
            # manual: -> https://docs.aws.amazon.com/eks/latest/userguide/create-kubeconfig.html
            aws eks update-kubeconfig --region ${AWS_REGION} --name ${EKS_CLUSTER_NAME}
            # check configuration
            kubectl config view
      - run:
          name: deploy container to Fargate cluster
          command: |
            # kubectl apply -f infrastructure/fargate_full.yaml
            cat infrastructure/fargate_full.yaml | sed "s/{{WORKFLOW_ID}}/${CIRCLE_WORKFLOW_ID:0:7}/g" | kubectl apply -f -
            # list all resources in namespace
            kubectl get all -n ml-app-${CIRCLE_WORKFLOW_ID:0:7}
            # check if deployment has completed
            kubectl -n ml-app-${CIRCLE_WORKFLOW_ID:0:7} rollout status deployment deployment-ml-app
            # check service details
            kubectl get service/service-ml-app -n ml-app-${CIRCLE_WORKFLOW_ID:0:7} 
            # check ingress status
            kubectl get ingress/ingress-ml-app -n ml-app-${CIRCLE_WORKFLOW_ID:0:7}
      - destroy-fargate-profile

  smoketest_app:
    docker:
      - image: amazon/aws-cli
    steps:
      - checkout
      - kubernetes/install
      - run:
          name: Create a kubeconfig for Amazon EKS 
          command: |
            # manual: -> https://docs.aws.amazon.com/eks/latest/userguide/create-kubeconfig.html
            aws eks update-kubeconfig --region ${AWS_REGION} --name ${EKS_CLUSTER_NAME}
            # check configuration
            kubectl config view
      - run:
          name: install jq for JSON parsing 
          command: |
            yum install jq -y
            jq -Version
      - run:
          name: get public IP of container from ingress and run smoketest
          command: |
            PUBLIC_IP=($(kubectl get ingress/ingress-ml-app -n ml-app-${CIRCLE_WORKFLOW_ID:0:7} | awk '$1=="ingress-ml-app"{print $4}'))
            cd test
            . smoketest.sh ${PUBLIC_IP} 80
       - destroy-fargate-profile

  # use cloudfront for production container
  prod_promotion:  
    docker:
      - image: amazon/aws-cli
    steps:
      - checkout
      - kubernetes/install
      - run:
          name: Create a kubeconfig for Amazon EKS 
          command: |
            # manual: -> https://docs.aws.amazon.com/eks/latest/userguide/create-kubeconfig.html
            aws eks update-kubeconfig --region ${AWS_REGION} --name ${EKS_CLUSTER_NAME}
            # check configuration
            kubectl config view
      - run:
          name: update cloudfront distribution
          command: |
            PUBLIC_IP=($(kubectl get ingress/ingress-ml-app -n ml-app-${CIRCLE_WORKFLOW_ID:0:7} | awk '$1=="ingress-ml-app"{print $4}'))
            echo "PUBLIC_IP: ${PUBLIC_IP}"
            aws cloudformation deploy \
            --template-file infrastructure/cloudfront.yml \
            --stack-name CloudFrontStack \
            --parameter-overrides PublicIP=${PUBLIC_IP} \
            --tags project=ml-app-${CIRCLE_WORKFLOW_ID:0:7}
            echo "Domain name and of ML-APP Cloudfront distribution:"
            aws cloudfront list-distributions --query "DistributionList.Items[].{DomainName: DomainName, Id: Origins.Items[0].Id}[?contains(Id, 'ml-app-api')] | [0]" --output text
      - run:
          name: delete cloudfront stack on fail
          when: on_fail
          command: |
            aws cloudformation delete-stack --stack-name CloudFrontStack
      - destroy-fargate-profile
            
  # ToDo
  # delete no longer required resources
  cleanup:
    docker:
      - image: amazon/aws-cli
    steps:
      - checkout
      - run:
          name: Install dependencies
          command: |
            # install tar utility
            yum install -y tar gzip
      - aws-eks/setup:
          version: 0.80.0
      - kubernetes/install
      - run:
          name: check installs
          command: |
            eksctl version
            kubectl config view
      - run:
          name: Get old Fargate profile workflow id, remove old Fargate profile
          command: |
            echo CIRCLE_WORKFLOW_ID "${CIRCLE_WORKFLOW_ID:0:7}"
            # Fetch the fargate profiles
            PROFILES=($(aws eks list-fargate-profiles \
            --cluster-name ${EKS_CLUSTER_NAME} \
            --region ${AWS_REGION} \
            --no-paginate \
            --output text | awk '{print $2}'))
            # remove default profile from list
            PROFILES=(${PROFILES[@]/fp-default})
            echo profile names: "${PROFILES[@]}"
            for PROFILE in "${PROFILES[@]}"
            do
              OldWorkflowID=${PROFILE: -7}
              echo OldWorkflowID: "${OldWorkflowID}"
              if [[ "${OldWorkflowID}" != "${CIRCLE_WORKFLOW_ID:0:7}" ]]
              then
                echo removing AWS Fargate profile of OldWorkflowID: "${OldWorkflowID}"
                eksctl delete fargateprofile \
                --name fargate_profile_ml-app-${OldWorkflowID} \
                --cluster ${EKS_CLUSTER_NAME} \
                --region ${AWS_REGION}
                # wait for 3 minutes since only one fargate profile can be deleted at the same time
                sleep 3m
              else
                echo "----------------- no old resources found -----------------"
              fi
            done  

workflows:
  default:
    jobs:
      # Testing
      # - build_container

      - build_app
      # - test_app # optional
      - build_container:
          requires: [build_app]
      # - scan_container # optional
      - create_eks_cluster:
          requires: [build_container]
          filters:
            branches:
              only: [master]
      - setup_eks_loadbalancer:
          requires: [create_eks_cluster]
      - create_fargate_profile:
          requires: [setup_eks_loadbalancer]
      - deploy_container:
          requires: [create_fargate_profile, build_container]
      - smoketest_app:
          requires: [deploy_container]
      - prod_promotion:
          requires: [smoketest_app]
      - cleanup:
          requires: [prod_promotion]
